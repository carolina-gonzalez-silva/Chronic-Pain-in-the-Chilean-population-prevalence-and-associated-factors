{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_2016 = pd.read_excel('DataBase/ENS_chile_2016-2017_expansion_factors.xlsx')\n",
    "ens_2016 = ens_2016.iloc[:,1::]\n",
    "ens_2016;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_2016['chronic_pain'] = np.where(ens_2016['pain_time_overall'] >= 90, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_2016 = ens_2016[ens_2016['age'] <= 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_2016 = ens_2016[ens_2016['has_cancer'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_2016['reduc_mob'] = ens_2016['reduc_mob'].apply(lambda x: 1 if x == 1 else 0)\n",
    "ens_2016['depression'] = ens_2016['depression'].apply(lambda x: 1 if x == 1 else 0)\n",
    "ens_2016['anxiety'] = ens_2016['anxiety'].apply(lambda x: 1 if x == 1 else 0)\n",
    "ens_2016['malnutrition'] = ens_2016['malnutrition'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ens_2016[['chronic_pain', 'sex', 'age', 'reduc_mob', 'depression', 'anxiety', 'malnutrition', 'educ_yrears', 'SES', 'factor']]\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e890da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df['chronic_pain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, mean_squared_error\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "from sklearn.linear_model import Lasso, MultiTaskLasso, Ridge, ElasticNet\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d10c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_ROC(X_, y, fig_show = True, params_ = None, rand_s = 10):\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.1, random_state=rand_s, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_train_weights =  X_train['factor']\n",
    "    X_test_weights =  X_test['factor']\n",
    "    \n",
    "    \n",
    "    #X_train = X_train[['sex', 'age', 'educ_yrears', 'total_local_pain_without_lat', 'SES']]\n",
    "    #X_test  = X_test[['sex', 'age', 'educ_yrears', 'total_local_pain_without_lat', 'SES']]\n",
    "    \n",
    "\n",
    "\n",
    "    #lista = list(sfs_.k_feature_names_)\n",
    "\n",
    "    #X_train = X_train[lista]\n",
    "    #X_test = X_test[lista]\n",
    "\n",
    "    cv    = RepeatedKFold(n_splits=3, n_repeats=2, random_state=101)\n",
    "    folds = [(train,test) for train, test in cv.split(X_train, y_train)]\n",
    "    metrics = ['auc', 'fpr', 'tpr', 'thresholds', 'f1_score', 'accuracy_score', 'recall_score', 'precision_score', 'confusion_matrix', 'model']\n",
    "    results = {\n",
    "        'train': {m:[] for m in metrics},\n",
    "        'val'  : {m:[] for m in metrics},\n",
    "        'test' : {m:[] for m in metrics}\n",
    "    }\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "\n",
    "\n",
    "    if(params_!=None):\n",
    "        params = params_\n",
    "    else:\n",
    "        params = {\n",
    "            'objective'   : 'binary:logistic',\n",
    "            'eval_metric' : 'logloss'\n",
    "        }\n",
    "\n",
    "\n",
    "    y_preds_ = []\n",
    "    labels_ = []\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, weight=X_test_weights)\n",
    "    for train, test in tqdm(folds, total=len(folds)):\n",
    "        \n",
    "        #print(X_train_weights, type(X_train_weights), X_train_weights.iloc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train.iloc[train,:], label=y_train.iloc[train], weight=X_train_weights.iloc[train])\n",
    "        dval   = xgb.DMatrix(X_train.iloc[test,:], label=y_train.iloc[test], weight=X_train_weights.iloc[test])\n",
    "        model  = xgb.train(\n",
    "            dtrain                = dtrain,\n",
    "            params                = params, \n",
    "            evals                 = [(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round       = 1000,\n",
    "            verbose_eval          = False,\n",
    "            early_stopping_rounds = 10,\n",
    "        )\n",
    "        sets = [dtrain, dval, dtest]\n",
    "\n",
    "        for i,ds in enumerate(results.keys()):\n",
    "            y_preds              = model.predict(sets[i])\n",
    "            labels               = sets[i].get_label()\n",
    "            fpr, tpr, thresholds = roc_curve(labels, y_preds)\n",
    "            results[ds]['fpr'].append(fpr)\n",
    "            results[ds]['tpr'].append(tpr)\n",
    "            results[ds]['thresholds'].append(thresholds)\n",
    "            results[ds]['auc'].append(roc_auc_score(labels, y_preds))\n",
    "\n",
    "            results[ds]['f1_score'].append(f1_score(labels, np.round(y_preds)))\n",
    "            results[ds]['accuracy_score'].append(accuracy_score(labels, np.round(y_preds)))\n",
    "            results[ds]['recall_score'].append(recall_score(labels, np.round(y_preds)))\n",
    "            results[ds]['precision_score'].append(precision_score(labels, np.round(y_preds)))\n",
    "            results[ds]['confusion_matrix'].append(confusion_matrix(labels, np.round(y_preds)))\n",
    "            results[ds]['model'].append(model)\n",
    "            \n",
    "            \n",
    "            if(ds == 'test'):\n",
    "                y_preds_.extend(y_preds)\n",
    "                labels_.extend(labels)\n",
    "                \n",
    "   # display(y_preds_)\n",
    "   # display(labels_)\n",
    "    print(roc_auc_score(labels_, y_preds_), rand_s)\n",
    "    \n",
    "    kind = 'test'\n",
    "    c_fill      = 'rgba(52, 152, 219, 0.2)'\n",
    "    c_line      = 'rgba(52, 152, 219, 0.5)'\n",
    "    c_line_main = 'rgba(41, 128, 185, 1.0)'\n",
    "    c_grid      = 'rgba(189, 195, 199, 0.5)'\n",
    "    c_annot     = 'rgba(149, 165, 166, 0.5)'\n",
    "    c_highlight = 'rgba(192, 57, 43, 1.0)'\n",
    "\n",
    "    fpr_mean    = np.linspace(0, 1, 100)\n",
    "    interp_tprs = []\n",
    "\n",
    "    for i in range(3*2):\n",
    "        fpr           = results[kind]['fpr'][i]\n",
    "        tpr           = results[kind]['tpr'][i]\n",
    "        interp_tpr    = np.interp(fpr_mean, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        interp_tprs.append(interp_tpr)\n",
    "\n",
    "    tpr_mean     = np.mean(interp_tprs, axis=0)\n",
    "    tpr_mean[-1] = 1.0\n",
    "    tpr_std      = 2*np.std(interp_tprs, axis=0)\n",
    "    tpr_upper    = np.clip(tpr_mean+tpr_std, 0, 1)\n",
    "    tpr_lower    = tpr_mean-tpr_std\n",
    "    auc          = np.mean(results[kind]['auc'])\n",
    "    auc_std          = np.std(results[kind]['auc'])\n",
    "    \n",
    "    roc_dict = {}\n",
    "    roc_dict['tpr_mean'] = tpr_mean\n",
    "    roc_dict['tpr_std'] = tpr_std\n",
    "    roc_dict['tpr_upper'] = tpr_upper\n",
    "    roc_dict['tpr_lower'] = tpr_lower\n",
    "    roc_dict['auc'] = auc\n",
    "    roc_dict['auc_std'] = auc_std\n",
    "\n",
    "    if(fig_show):\n",
    "        fig = go.Figure([\n",
    "            go.Scatter(\n",
    "                x          = fpr_mean,\n",
    "                y          = tpr_upper,\n",
    "                line       = dict(color=c_line, width=1),\n",
    "                hoverinfo  = \"skip\",\n",
    "                showlegend = False,\n",
    "                name       = 'upper'),\n",
    "            go.Scatter(\n",
    "                x          = fpr_mean,\n",
    "                y          = tpr_lower,\n",
    "                fill       = 'tonexty',\n",
    "                fillcolor  = c_fill,\n",
    "                line       = dict(color=c_line, width=1),\n",
    "                hoverinfo  = \"skip\",\n",
    "                showlegend = False,\n",
    "                name       = 'lower'),\n",
    "            go.Scatter(\n",
    "                x          = fpr_mean,\n",
    "                y          = tpr_mean,\n",
    "                line       = dict(color=c_line_main, width=2),\n",
    "                hoverinfo  = \"skip\",\n",
    "                showlegend = True,\n",
    "                name       = f'AUC: {auc:.3f}')\n",
    "        ])\n",
    "\n",
    "        fig.add_shape(\n",
    "            type ='line', \n",
    "            line =dict(dash='dash'),\n",
    "            x0=0, x1=1, y0=0, y1=1\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            template    = 'plotly_white', \n",
    "            title_x     = 0.5,\n",
    "            xaxis_title = \"1 - Specificity\",\n",
    "            yaxis_title = \"Sensitivity\",\n",
    "            width       = 800,\n",
    "            height      = 800,\n",
    "            legend      = dict(\n",
    "                yanchor=\"bottom\", \n",
    "                xanchor=\"right\", \n",
    "                x=0.95,\n",
    "                y=0.01,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(\n",
    "            range       = [0, 1],\n",
    "            gridcolor   = c_grid,\n",
    "            scaleanchor = \"x\", \n",
    "            scaleratio  = 1,\n",
    "            linecolor   = 'black')\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            range       = [0, 1],\n",
    "            gridcolor   = c_grid,\n",
    "            constrain   = 'domain',\n",
    "            linecolor   = 'black')\n",
    "\n",
    "\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "    else:\n",
    "        print(y.name, 'AUC: ', np.round(auc,2), '+-( ', np.round(np.std(results[kind]['auc']),2), ')', X_.shape)\n",
    "        print('acc={:.2f} prec={:.2f}  f1={:.2f} rec={:.2f}'.format(np.mean(results[ds]['accuracy_score']), np.mean(results[ds]['precision_score']),np.mean(results[ds]['f1_score']), np.mean(results[ds]['recall_score'])))\n",
    "\n",
    "    return (model, results, np.round(auc,2), roc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "def SearchBestModel(case_x, case_y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(case_x, case_y, test_size=.1, random_state=1)\n",
    "\n",
    "    opt_XGB = BayesSearchCV(\n",
    "            XGBClassifier(),\n",
    "            {\n",
    "                'objective'   : ['binary:logistic'],\n",
    "                'eval_metric' : ['logloss', 'error', 'auc', 'aucpr'],\n",
    "                'learning_rate': (0.01, 0.1, 0.2),\n",
    "                'max_depth': (3, 4, 5),\n",
    "                'subsample': (0.8, 0.9, 1.0),\n",
    "                'colsample_bytree': (0.8, 0.9, 1.0),\n",
    "                'reg_alpha': (0, 0.1, 1.0),\n",
    "                'reg_lambda': (0, 0.1, 1.0),\n",
    "                #'gamma': (0.001, 0.01, 0.1, 1, 10),\n",
    "            },\n",
    "                          \n",
    "            #n_iter=10,\n",
    "            #cv=3,\n",
    "            random_state=1\n",
    "        )\n",
    "\n",
    "    opt_XGB.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print('XGB')\n",
    "    print(\"val. score: %s\" % opt_XGB.best_score_)\n",
    "    print(\"test score: %s\" % opt_XGB.score(X_test, y_test))\n",
    "    print(\"best parameters: %s\" % str(opt_XGB.best_params_))\n",
    "    print('---------------------------------------------\\n')\n",
    "    \n",
    "    return opt_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feat_import(gini_results_metric, top_10_keys_metric, colores = ['#f25c05', '#9c0720'], gini_plus_var = True, title = ''):\n",
    "\n",
    "    count = 1\n",
    "    for i in top_10_keys_metric:\n",
    "\n",
    "        #auc = gini_results_metric[i][3]['auc']\n",
    "#\n",
    "        #acc = np.mean(gini_results_metric[i][1]['test']['accuracy_score'])\n",
    "        #prec = np.mean(gini_results_metric[i][1]['test']['precision_score'])\n",
    "        #f1 = np.mean(gini_results_metric[i][1]['test']['f1_score'])\n",
    "        \n",
    "        \n",
    "        auc = np.mean(gini_results_metric[i][1]['test']['auc'])\n",
    "\n",
    "        acc = np.mean(gini_results_metric[i][1]['test']['accuracy_score'])\n",
    "        prec = np.mean(gini_results_metric[i][1]['test']['precision_score'])\n",
    "        f1 = np.mean(gini_results_metric[i][1]['test']['f1_score'])\n",
    "        rec = np.mean(gini_results_metric[i][1]['test']['recall_score'])\n",
    "\n",
    "        auc_std = np.std(gini_results_metric[i][1]['test']['auc'])\n",
    "        acc_std = np.std(gini_results_metric[i][1]['test']['accuracy_score'])\n",
    "        prec_std = np.std(gini_results_metric[i][1]['test']['precision_score'])\n",
    "        f1_std = np.std(gini_results_metric[i][1]['test']['f1_score'])\n",
    "        rec_std = np.std(gini_results_metric[i][1]['test']['recall_score'])\n",
    "        \n",
    "        conf_matrix = gini_results_metric[i][1]['test']['confusion_matrix']\n",
    "\n",
    "        conf_matrix_mean = np.mean(conf_matrix, axis=0)\n",
    "        conf_matrix_std  = np.std(conf_matrix, axis=0)\n",
    "\n",
    "        fscore = {}\n",
    "        for k in ['sex', 'age', 'reduc_mob', 'depression', 'anxiety', 'malnutrition', 'educ_yrears', 'SES']:\n",
    "            fscore[k] = []\n",
    "\n",
    "        for model in gini_results_metric[i][1]['test']['model']:\n",
    "\n",
    "            for j in ['sex', 'age', 'reduc_mob', 'depression', 'anxiety', 'malnutrition', 'educ_yrears', 'SES']:\n",
    "                try:\n",
    "                    fscore[j].append(model.get_fscore()[j])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "        if(count<=4):\n",
    "            plt.subplot(2,1,count)   \n",
    "            max_length = max(len(v) for v in fscore.values())\n",
    "\n",
    "            fscore_ = {k: v + [None] * (max_length - len(v)) for k, v in fscore.items()}\n",
    "\n",
    "            summary_df = pd.DataFrame(fscore_)\n",
    "\n",
    "            summary_df = pd.DataFrame({\n",
    "                'Mean': summary_df.mean(),\n",
    "                'Std': summary_df.std()\n",
    "            })\n",
    "\n",
    "\n",
    "            summary_df = summary_df.sort_values(by='Mean', ascending=True)\n",
    "            \n",
    "            \n",
    "            mean_values = summary_df.Mean\n",
    "            std_values = summary_df.Std\n",
    "            \n",
    "            #display(mean_values)\n",
    "            #display(std_values)\n",
    "            \n",
    "            ax = mean_values.plot(kind='barh', xerr=std_values, capsize=4, color=colores)\n",
    "\n",
    "            plt.ylabel('Features')\n",
    "            plt.xlabel('f-score')\n",
    "            plt.title(''+i + '\\n' + ' auc={:.2f}\\n acc={:.2f} prec={:.2f} \\n f1={:.2f} rec={:.2f}'.format(auc, acc,prec, f1, rec))\n",
    "\n",
    "            xlimits = ax.get_xlim()\n",
    "            ax.set_xticks([xlimits[0], xlimits[1]])\n",
    "            ax.tick_params(axis='x', labelsize=14)\n",
    "            \n",
    "            n_row, n_col = conf_matrix_mean.shape\n",
    "            labels = [[f\"{conf_matrix_mean[i, j]:.2f} \\n ({conf_matrix_std[i, j]:.2f})\" for j in range(n_col)] for i in range(n_row)]\n",
    "\n",
    "\n",
    "            clases = ['No chronic pain', 'Chronic pain']\n",
    "\n",
    "            plt.subplot(2,1,count+1) \n",
    "            plt.imshow(conf_matrix_mean, interpolation='nearest', cmap=plt.cm.Greys)\n",
    "            plt.title(''+i)\n",
    "            plt.colorbar()\n",
    "\n",
    "            tick_marks = np.arange(len(clases))\n",
    "            plt.xticks(tick_marks, clases)\n",
    "            plt.yticks(tick_marks, clases)\n",
    "\n",
    "            plt.ylabel('True')\n",
    "            plt.xlabel('Predict')\n",
    "\n",
    "            for row_ in range(n_row):\n",
    "                for col_ in range(n_col):\n",
    "                    plt.text(col_, row_, labels[row_][col_], verticalalignment=\"center\", \n",
    "                             horizontalalignment=\"center\", color=\"white\" if conf_matrix_mean[row_, col_] > 30 else \"black\")\n",
    "\n",
    "\n",
    "        count+=1\n",
    "        \n",
    "        metrics = {\n",
    "            'AUC': [auc, auc_std],\n",
    "            'Accuracy': [acc, acc_std],\n",
    "            'Precision': [prec, prec_std],\n",
    "            'F1 Score': [f1, f1_std],\n",
    "            'Recall': [rec, rec_std]\n",
    "        }\n",
    "        df_metrics = pd.DataFrame(metrics, index=['Mean', 'STD'])\n",
    "\n",
    "\n",
    "\n",
    "        clases = ['No chronic pain', 'Chronic pain']\n",
    "\n",
    "\n",
    "        df_conf_m = pd.DataFrame(conf_matrix_mean, index=clases, columns=clases)\n",
    "        df_conf_s = pd.DataFrame(conf_matrix_std, index=clases, columns=clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_roc(gini_results_metric, top_10_keys_metric, title = ''):\n",
    "    #metric_tpr_mean = np.zeros_like(gini_results_metric[i][3]['tpr_mean'])\n",
    "\n",
    "    metric_tpr_mean = 0\n",
    "    metric_auc_roc_list = []\n",
    "    count = 0\n",
    "    last_i = ''\n",
    "\n",
    "    acc_list = []\n",
    "    prec_list = []\n",
    "    f1_list=[]\n",
    "    rec_list = []\n",
    "    for i in top_10_keys_metric:\n",
    "\n",
    "\n",
    "        tpr = gini_results_metric[i][3]['tpr_mean']\n",
    "        fpr = np.linspace(0, 1, 100)\n",
    "        auc_roc = gini_results_metric[i][3]['auc']\n",
    "        auc_roc_std = gini_results_metric[i][3]['auc_std']\n",
    "\n",
    "        if(auc_roc<0.50): # Ya los que son menos de 0.75 no los tengo en cuenta así que pinto de naranja el anterior y no guardo más valores para no afectar los promedios\n",
    "            tpr = gini_results_metric[last_i][3]['tpr_mean']\n",
    "            fpr = np.linspace(0, 1, 100)\n",
    "            auc_roc = gini_results_metric[last_i][3]['auc']\n",
    "            plt.plot(fpr, tpr, color='#FA7F08', alpha=1, lw=1, label='(AUC = {:.2f}) ({:.3f})'.format(auc_roc, auc_roc_std))\n",
    "            \n",
    "            l = len(metric_auc_roc_list)\n",
    "            plt.plot(fpr, metric_tpr_mean/l, color='#0D0D0C', alpha=1, lw=2, label='(AUC = {:.2f} [{:.3f}])'.format(np.mean(metric_auc_roc_list), np.std(metric_auc_roc_list)))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "\n",
    "            title_metrics = '\\n' + 'acc'\n",
    "            plt.title('ROC ( '+title+')\\n acc={:.2f}  prec={:.2f}  f1={:.2f}'.format(np.mean(acc_list), \n",
    "                                                                                        np.mean(prec_list),\n",
    "                                                                                        np.mean(f1_list)))\n",
    "            return\n",
    "\n",
    "        if count==0:\n",
    "            plt.plot(fpr, tpr, color='#F24405', alpha=1, lw=1, label='(AUC = {:.2f} [{:.3f}]) '.format(auc_roc, auc_roc_std))\n",
    "        elif count==(len(top_10_keys_metric)-1): \n",
    "            plt.plot(fpr, tpr, color='#FA7F08', alpha=1, lw=1, label='(AUC = {:.2f} [{:.3f}])'.format(auc_roc, auc_roc_std))\n",
    "        else:\n",
    "            plt.plot(fpr, tpr, color='#747E7E', alpha=0.55, lw=0.5)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=1.5)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "        metric_tpr_mean+=tpr\n",
    "        metric_auc_roc_list.append(auc_roc)\n",
    "\n",
    "        acc_list.append(np.mean(gini_results_metric[i][1]['test']['accuracy_score']))\n",
    "        prec_list.append(np.mean(gini_results_metric[i][1]['test']['precision_score']))\n",
    "        f1_list.append(np.mean(gini_results_metric[i][1]['test']['f1_score']))\n",
    "        rec_list.append(np.mean(gini_results_metric[i][1]['test']['recall_score']))\n",
    "        count+=1\n",
    "        last_i = i\n",
    "\n",
    "   #l = len(metric_auc_roc_list)\n",
    "   #plt.plot(fpr, metric_tpr_mean/l, color='#0D0D0C', alpha=1, lw=2, label='(AUC = {:.2f} [{:.2f}])'.format(np.mean(metric_auc_roc_list), np.std(metric_auc_roc_list)))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    title_metrics = '\\n' + 'acc'\n",
    "    plt.title('ROC ('+title+')\\n acc={:.2f}  prec={:.2f}  f1={:.2f} rec={:.2f}'.format(np.mean(acc_list), \n",
    "                                                                                np.mean(prec_list),\n",
    "                                                                                np.mean(f1_list), np.mean(rec_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = df['chronic_pain']\n",
    "X_ = df.drop(['chronic_pain'], axis=1)\n",
    "\n",
    "\n",
    "columns_to_normalize = ['sex', 'age', 'reduc_mob', 'depression', 'anxiety', 'malnutrition', 'educ_yrears', 'SES']\n",
    "scaler = MinMaxScaler([0.05, 0.95])\n",
    "\n",
    "X_s = X_.copy()\n",
    "X_s[columns_to_normalize]  = scaler.fit_transform(X_[columns_to_normalize])\n",
    "#X_s = pd.DataFrame(scaling_data, columns= X_.columns, index = X_.index)\n",
    "\n",
    "X_ =  X_s.copy()\n",
    "\n",
    "opt_XGB = SearchBestModel(X_.drop(['factor'], axis = 1), y_)\n",
    "params_ = dict(opt_XGB.best_params_)\n",
    "X_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy='auto', random_state=10)\n",
    "X_resampled, y_resampled = undersample.fit_resample(X_, y_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b9c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    try:\n",
    "        classifier_ROC(X_resampled, y_resampled, False, None, i);\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55215f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = classifier_ROC(X_resampled, y_resampled, False, None, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb230da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pain_results = {}\n",
    "pain_results['chronic_pain'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a516c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "graph_roc(pain_results, ['chronic_pain'], title = 'NHS 2016')\n",
    "plt.savefig('Paper_figures/pdfs/ROC_2016.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d91646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,8))\n",
    "graph_feat_import(pain_results, ['chronic_pain'], colores=['gray'])\n",
    "plt.savefig('Paper_figures/pdfs/feat_2016.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192ae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6aece7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
